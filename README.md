# Laboratorio 2: Clasificaci√≥n de Sentimientos con Transformers

Este proyecto es una implementaci√≥n de un pipeline de Machine Learning para la **clasificaci√≥n binaria de sentimientos** (positivo/negativo) de rese√±as de pel√≠culas del dataset IMDB.

El objetivo principal es construir los componentes clave de un modelo Transformer desde cero, incluyendo un tokenizador BPE (Byte-Pair Encoding) personalizado y el mecanismo de Multi-Head Attention, para luego entrenarlo y evaluar su rendimiento.

El modelo final alcanza una **precisi√≥n de ~89.7%** en el conjunto de pruebas.

## Caracter√≠sticas Principales

* **An√°lisis Exploratorio de Datos (EDA):** Visualizaci√≥n de la distribuci√≥n de longitudes de rese√±as y balance de clases.
* **Tokenizador BPE Personalizado:** Implementaci√≥n de un tokenizador Byte-Pair Encoding desde cero, que construye un vocabulario de 30,000 sub-palabras a partir del corpus de IMDB.
* **Atenci√≥n Multi-Cabeza (Multi-Head Attention):** Implementaci√≥n personalizada de las capas `ScaledDotProductAttention` y `MultiHeadAttention`.
* **Arquitectura Transformer Encoder:** Construcci√≥n de un modelo apilando m√∫ltiples capas de `TransformerEncoderLayer` (incluyendo conexiones residuales, LayerNorm y FFN).
* **Entrenamiento y Evaluaci√≥n:**
    * Uso del optimizador `AdamW` y un scheduler `ReduceLROnPlateau` para un entrenamiento estable.
    * Implementaci√≥n de **Early Stopping** para prevenir el sobreajuste.
    * An√°lisis de rendimiento detallado en el conjunto de prueba, incluyendo una matriz de confusi√≥n y un desglose de la precisi√≥n por longitud de rese√±a.

## C√≥mo Empezar

### Prerrequisitos

* Python 3.8+
* PyTorch
* Jupyter Notebook, Jupyter Lab o Google Colab

### Instalaci√≥n

1.  Clona este repositorio (o descarga los archivos):
    ```bash
    git clone [https://github.com/tu-usuario/tu-repositorio.git](https://github.com/tu-usuario/tu-repositorio.git)
    cd tu-repositorio
    ```

2.  Crea un entorno virtual (recomendado):
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # En Windows: .venv\Scripts\activate
    ```

3.  Instala las dependencias necesarias:
    ```bash
    pip install torch datasets numpy scikit-learn matplotlib seaborn
    ```

### üèÉ Ejecuci√≥n

1.  Abre el Jupyter Notebook:
    ```bash
    jupyter lab "Lab2 2025.ipynb"
    ```

2.  **Importante! Activa la GPU:**
    * **En Google Colab:** Ve a `Entorno de ejecuci√≥n` ‚Üí `Cambiar tipo de entorno de ejecuci√≥n` y selecciona `T4 GPU` como acelerador de hardware.
    * **Localmente:** Aseg√∫rate de tener una GPU compatible con CUDA y que PyTorch la est√© detectando.

3.  Ejecuta todas las celdas en orden. El script har√° lo siguiente:
    * Cargar√° y analizar√° los datos.
    * **Construir√° el vocabulario BPE de 30,000 tokens** (esto puede tardar varios minutos).
    * Definir√° la arquitectura del modelo.
    * Entrenar√° el modelo, guardando el mejor checkpoint como `best_model.pth`.
    * Evaluar√° el modelo en el conjunto de prueba.
    * Ejecutar√° pruebas con ejemplos personalizados.

## üß† Detalles de Implementaci√≥n

### 1. Tokenizador BPE (`SimpleTokenizer`)

Se implement√≥ un tokenizador BPE desde cero. En lugar de usar un tokenizador pre-entrenado, este:
1.  **Limpia** los textos y cuenta la frecuencia de las palabras (a√±adiendo `</w>` al final).
2.  **Inicializa** el vocabulario con todos los caracteres base.
3.  **Iterativamente** cuenta los pares de s√≠mbolos m√°s frecuentes y los fusiona.
4.  **Aprende** 30,000 "merges" (fusiones) para crear un vocabulario de sub-palabras.
5.  Proporciona m√©todos `encode()` y `decode()` que utilizan estos "merges" aprendidos.

### 2. Modelo Transformer

El modelo (`SentimentAnalysisModel`) no utiliza `nn.TransformerEncoder`, sino que construye la pila manualmente:

* **Embedding + Positional Encoding:** Convierte los IDs de los tokens en vectores y les suma la informaci√≥n posicional.
* **Pila de N Capas Encoder:** El modelo utiliza **4 capas** (`num_layers = 4`) de `TransformerEncoderLayer`.
* **Capa Encoder Personalizada:** Cada `TransformerEncoderLayer` contiene:
    1.  Una capa `MultiHeadAttention` personalizada (**8 cabezas**, `num_heads = 8`).
    2.  Una conexi√≥n residual y `LayerNorm`.
    3.  Una red FeedForward (FFN).
    4.  Otra conexi√≥n residual y `LayerNorm`.
* **Clasificaci√≥n:** La salida de la secuencia del Transformer (shape `[Batch, SeqLen, DimModel]`) se promedia a lo largo de la dimensi√≥n de la secuencia (`.mean(dim=1)`) para obtener un vector de sentimiento √∫nico por rese√±a, que luego se pasa a una capa lineal final para la clasificaci√≥n.

## Resultados

Tras el entrenamiento con *Early Stopping*, el modelo detuvo su entrenamiento en la **√âpoca 6** al no encontrar mejoras en la p√©rdida de validaci√≥n.

* **Precisi√≥n de Prueba (Test Accuracy):** **89.66%**
* **Mejor P√©rdida de Validaci√≥n:** 0.3076 (alcanzada en la √âpoca 1)

### Matriz de Confusi√≥n (Prueba)

| | Pred. Negativo | Pred. Positivo |
| :--- | :---: | :---: |
| **Real Negativo**| 2235 | 277 |
| **Real Positivo**| 240 | 2248 |

### An√°lisis por Longitud de Rese√±a

El modelo demostr√≥ ser robusto independientemente de la longitud del texto, gracias al `max_length=512` y al mecanismo de atenci√≥n.

* **Rese√±as Cortas (<150 palabras):** 89.86%
* **Rese√±as Medianas (150-300 palabras):** 89.33%
* **Rese√±as Largas (>=300 palabras):** 89.88%

### Prueba en Espa√±ol (Out-of-Distribution)

Como era de esperar, el modelo falla al clasificar texto en espa√±ol, ya que su vocabulario BPE se construy√≥ exclusivamente con el corpus en ingl√©s de IMDB.

* **Texto:** `La pelicula fue bastante aburrida, carec√≠a de buena trama`
* **Predicci√≥n:** **Positive** (Incorrecta)

Esto demuestra que el vocabulario es el componente fundamental que limita el modelo a un idioma espec√≠fico.
